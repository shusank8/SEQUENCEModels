{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHkbGZhoXXWDTs9SUgvBqt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shusank8/SEQUENCEModels/blob/main/LSTMFROMScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5TEwcygRhEEK",
        "outputId": "7a49d48e-874c-4e88-c36f-df836b1249b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIMPLE LONG SHORT TERM MEMORY; BY SHUSANKET BASYAL\n"
          ]
        }
      ],
      "source": [
        "print('SIMPLE LONG SHORT TERM MEMORY; BY SHUSANKET BASYAL')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING THE DATASET\n",
        "# DATASET IS THE SHORT JOKES FROM KAGGLE\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"abhinavmoudgil95/short-jokes\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "xH-cp_H5cd90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadcd94f-b779-49bf-fa7a-4277f18d0573"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/abhinavmoudgil95/short-jokes/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING THE NECESSARY LIBARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "O8J8T6i6d-Lb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOOKING WHERE THE FILES HAS BEEN DOWNLOADED\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "id": "xXC0tXrqeAhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445839bc-60fe-4492-ba8d-4f1fe1ea25d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['shortjokes.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING THE FILE INTO DF\n",
        "df = pd.read_csv(path+\"/shortjokes.csv\")\n",
        "# GETTING ALL THE VALUES IN JOKE COLUMN => RETURNS A LIST\n",
        "text = df['Joke'].values\n",
        "# JOINING ALL THE STR VAL IN THE LIST TO GET A SINGLE STR\n",
        "text = \"\".join(text)\n",
        "# GETTING THE UNIQUE CHAR PRESENT IN THE DATASET AND CREATING A VARIABLE VOCAB_SIZE THAT STORES THE LEN OF THE UNIQUE ELEMENTS\n",
        "char = sorted(list(set(text)))\n",
        "vocab_size = len(char)\n",
        "# SIMPLE ENCODER, DECODER\n",
        "# CREATING A HASMAP THAT MAPS STRING TO ID AND VICE VERSA\n",
        "stringtoid = {sti:i for i,sti in enumerate(char)}\n",
        "idtostring = {i:sti for i, sti in enumerate(char)}\n",
        "# USING THE CREATED HASMAP TO CREATER ENCODER AND DECODER\n",
        "encode = lambda x : [stringtoid[i] for i in x]\n",
        "decode = lambda x: \"\".join([idtostring[i] for i in x])\n",
        "# ENCODING THE TEXT\n",
        "text = torch.tensor(encode(text), dtype=torch.long)\n",
        "# CREATING TRAIN AND VAL SIZE\n",
        "n = int(0.8*len(text))\n",
        "train = text[0:n]\n",
        "val = text[n:]"
      ],
      "metadata": {
        "id": "HwvfoqxheBt7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function creates batches of data for training or validation.\n",
        "# It selects random starting points, extracts sequences of a given length (block_size), and prepares input (x) and target (y) tensors for a model.\n",
        "\n",
        "def generate_batch(split, batch_size, block_size):\n",
        "  data = train if split =='train' else val\n",
        "  idx = torch.randint(0, len(data)-block_size, (batch_size, ))\n",
        "  x = torch.stack([data[i:i+block_size] for i in idx])\n",
        "  y = torch.stack([data[i+1:i+1+block_size] for i in idx])\n",
        "  return x,y\n"
      ],
      "metadata": {
        "id": "xn9SvLnPeDbN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function estimates the model's loss on the validation set by running 64 mini-batches through it.\n",
        "# It calculates cross-entropy loss for each batch and returns the average loss, temporarily switching the model to evaluation mode for accurate assessment.\n",
        "\n",
        "def estimate_loss(model, vocab_size, batch_size, block_size):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  losses = torch.zeros(64)\n",
        "  for _ in range(64):\n",
        "    x,y = generate_batch('val', batch_size, block_size)\n",
        "    x = x.to('cuda')\n",
        "    y = y.to('cuda')\n",
        "    logits = model(x)\n",
        "    logits = logits.reshape(-1, vocab_size)\n",
        "    y = y.view(-1)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    losses[_] = loss.item()\n",
        "  model.train()\n",
        "  return losses.mean()\n"
      ],
      "metadata": {
        "id": "3x3mzFNeeFDr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embdim = 64\n",
        "block_size = 64\n",
        "hidim = 64\n",
        "# outdim = 32\n",
        "batch_size = 128\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MctWzmknVK3W",
        "outputId": "75649dfe-3316-4296-859c-2af02b7789b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMFROMScratch(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(vocab_size, embdim)\n",
        "\n",
        "    # forget gate\n",
        "    self.forget_gate = nn.Linear(hidim, hidim, bias = False)\n",
        "\n",
        "    # input gate\n",
        "    self.input_gate = nn.Linear(hidim, hidim, bias= False)\n",
        "\n",
        "    # candidate gate\n",
        "    self.candidate_gate = nn.Linear(hidim, hidim, bias = False)\n",
        "\n",
        "    # output gate\n",
        "    self.outputgate = nn.Linear(hidim,hidim, bias = False)\n",
        "\n",
        "    self.input_to_hidden = nn.Linear(embdim, hidim,bias = False)\n",
        "\n",
        "    self.hidden_to_hidden = nn.Linear(hidim, hidim, bias = False)\n",
        "\n",
        "    self.out = nn.Linear(hidim, vocab_size, bias = False)\n",
        "\n",
        "  def forward(self, x, h=None, c = None):\n",
        "    x = self.embeddings(x)\n",
        "    # shape of x => (B,T,C)\n",
        "    x = x.transpose(0,1)\n",
        "    T,B,C = x.shape\n",
        "    if h is None:\n",
        "      h = torch.zeros(B, hidim, device = 'cuda')\n",
        "      c = torch.zeros(B, hidim, device = 'cuda')\n",
        "    res = []\n",
        "    for _ in range(T):\n",
        "\n",
        "      xi = x[_]\n",
        "\n",
        "      a = self.input_to_hidden(xi)\n",
        "\n",
        "      b = self.hidden_to_hidden(h)\n",
        "\n",
        "      z = a+b\n",
        "\n",
        "      fg = torch.sigmoid(self.forget_gate(z))\n",
        "\n",
        "      ig = torch.sigmoid(self.input_gate(z))\n",
        "\n",
        "      cg = torch.tanh(self.candidate_gate(z))\n",
        "\n",
        "      c = c*fg + ig*cg\n",
        "\n",
        "      og = torch.sigmoid(self.outputgate(z))\n",
        "\n",
        "      h = torch.tanh(c)*og\n",
        "\n",
        "      ot = self.out(h)\n",
        "\n",
        "      res.append(ot)\n",
        "\n",
        "    res = torch.stack(res)\n",
        "\n",
        "    res = res.transpose(0,1)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2tAL3bEceJJJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTMPY(nn.Module):\n",
        "\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "#     self.embeddings = nn.Embedding(vocab_size, embdim)\n",
        "#     self.lstm = nn.LSTM(embdim, hidim, 1, True, True)\n",
        "#     self.out = nn.Linear(hidim, vocab_size, bias=False)\n",
        "#   def forward(self, x):\n",
        "#     x = self.embeddings(x)\n",
        "#     out,hid = self.lstm(x)\n",
        "#     return self.out(out)"
      ],
      "metadata": {
        "id": "oDlRWMAgUwyT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = LSTMPY()\n",
        "# for name, p in model.named_parameters():\n",
        "#   print(name, p.size())"
      ],
      "metadata": {
        "id": "UYBRNatkVeYQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMFROMScratch()\n",
        "for name, param in model.named_parameters():\n",
        "  if param.dim()>=2:\n",
        "    torch.nn.init.xavier_normal_(param)\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "IpLFwhKTVkIn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 10000\n",
        "for _ in range(epoches):\n",
        "\n",
        "  x,y = generate_batch('train', batch_size, block_size)\n",
        "  x = x.to(\"cuda\")\n",
        "  y = y.to(\"cuda\")\n",
        "  logits = model(x)\n",
        "  logits = logits.reshape(-1, vocab_size)\n",
        "  y = y.view(-1)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if _%200==0:\n",
        "\n",
        "    l = estimate_loss(model, vocab_size, batch_size, block_size)\n",
        "    print(\"step:\", _ , \"loss=>\", l.item())"
      ],
      "metadata": {
        "id": "JtwV2khXWUAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae24b283-9313-45bd-d41f-072bc6637d77"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 loss=> 1.8747875690460205\n",
            "step: 200 loss=> 1.8779109716415405\n",
            "step: 400 loss=> 1.8779385089874268\n",
            "step: 600 loss=> 1.867386817932129\n",
            "step: 800 loss=> 1.8672397136688232\n",
            "step: 1000 loss=> 1.8558533191680908\n",
            "step: 1200 loss=> 1.875618815422058\n",
            "step: 1400 loss=> 1.8613343238830566\n",
            "step: 1600 loss=> 1.8697429895401\n",
            "step: 1800 loss=> 1.86432683467865\n",
            "step: 2000 loss=> 1.8625586032867432\n",
            "step: 2200 loss=> 1.8558647632598877\n",
            "step: 2400 loss=> 1.8561317920684814\n",
            "step: 2600 loss=> 1.8602027893066406\n",
            "step: 2800 loss=> 1.8501415252685547\n",
            "step: 3000 loss=> 1.8560590744018555\n",
            "step: 3200 loss=> 1.8562830686569214\n",
            "step: 3400 loss=> 1.8516346216201782\n",
            "step: 3600 loss=> 1.8542227745056152\n",
            "step: 3800 loss=> 1.854975700378418\n",
            "step: 4000 loss=> 1.8534175157546997\n",
            "step: 4200 loss=> 1.8487434387207031\n",
            "step: 4400 loss=> 1.8441667556762695\n",
            "step: 4600 loss=> 1.8480756282806396\n",
            "step: 4800 loss=> 1.84722101688385\n",
            "step: 5000 loss=> 1.8460007905960083\n",
            "step: 5200 loss=> 1.8435312509536743\n",
            "step: 5400 loss=> 1.8417209386825562\n",
            "step: 5600 loss=> 1.838880181312561\n",
            "step: 5800 loss=> 1.8390052318572998\n",
            "step: 6000 loss=> 1.8404632806777954\n",
            "step: 6200 loss=> 1.834282398223877\n",
            "step: 6400 loss=> 1.843118667602539\n",
            "step: 6600 loss=> 1.8394720554351807\n",
            "step: 6800 loss=> 1.8410193920135498\n",
            "step: 7000 loss=> 1.8385543823242188\n",
            "step: 7200 loss=> 1.8321877717971802\n",
            "step: 7400 loss=> 1.8392523527145386\n",
            "step: 7600 loss=> 1.8380999565124512\n",
            "step: 7800 loss=> 1.8343912363052368\n",
            "step: 8000 loss=> 1.8318060636520386\n",
            "step: 8200 loss=> 1.831523060798645\n",
            "step: 8400 loss=> 1.8339924812316895\n",
            "step: 8600 loss=> 1.8349452018737793\n",
            "step: 8800 loss=> 1.8266091346740723\n",
            "step: 9000 loss=> 1.8338018655776978\n",
            "step: 9200 loss=> 1.8276692628860474\n",
            "step: 9400 loss=> 1.8302232027053833\n",
            "step: 9600 loss=> 1.8311455249786377\n",
            "step: 9800 loss=> 1.8287761211395264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This function generates tokens using the trained model.\n",
        "# Starting from a given input, it predicts the next token, samples from the probability distribution, appends it to the sequence,\n",
        "# and continues for max_tok steps without updating gradients.\n",
        "\n",
        "def generatetok(model, start, max_tok):\n",
        "  with torch.no_grad():\n",
        "    for _ in range(max_tok):\n",
        "      # start2 = start[:, -block_size:, :]\n",
        "      B,T = start.shape\n",
        "      logits = model(start)\n",
        "\n",
        "      # logits = logits.reshape(-1, vocab_size)\n",
        "      prob = logits[:,-1,:]\n",
        "      prob = F.softmax(prob, dim=-1)\n",
        "      lo = torch.multinomial(prob, num_samples=1)\n",
        "      start = torch.cat([start, lo], dim=1)\n",
        "  return start\n"
      ],
      "metadata": {
        "id": "dEh5ca_waIm1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = torch.tensor([2,2,2], device='cuda',dtype=torch.long).reshape(3,1)"
      ],
      "metadata": {
        "id": "419yzLguas7x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZING THE START AS 0\n",
        "# start = torch.zeros([3,1], device='cuda',dtype=torch.long)\n",
        "# GENERATING FROM THE MODEL\n",
        "out = generatetok(model, start, 256)\n",
        "out.shape\n",
        "# output\n",
        "res = []\n",
        "for _ in range(start.shape[0]):\n",
        "  o = out[_]\n",
        "  res.append(decode(o.tolist()))\n",
        "for x in res:\n",
        "  print(x)\n",
        "  print(\"--------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QYialoqavPV",
        "outputId": "aca53e4a-aeeb-4a0d-c269-adbb05cd2d6c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " it yous? Becoy of the girl 2.? Intle with bal same his it : excite wallber neht thought it a pus the dear faviel wa horm.What do you call bird an one? Stime run. In does -I houges on asdap of or Trurpodrur lomn of theirbause one it. OK it.I meen Matuciots \n",
            "--------------\n",
            "\n",
            " look's hors in I thi Mome no workfaty my greez once who go pightter three are in pies comes her Say the offtivite lose to drom\" play Alutber's feting vause I be the how who can kids frienses with, shoots ttatinaly quour mach can dad I fairl a when yell it \n",
            "--------------\n",
            "\n",
            " the jush you side.Well scomons.Why is with just no bunge into you ups the breating geet will resturning GMGUDKY  CAK. That's just people Cacked Are that overble to got cub alouther to see 4.Sahtbarl like A*I just put then cotle.I dad of hord me the bording\n",
            "--------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9KB6gh6Ba6jg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}